# Contexto: Otimização de Recall RAG

## Problema Original
Os casos de teste para CIPA, EPI, Trabalho Portuário (NR-29) e Aquaviário (NR-30) apresentavam Recall baixo ou zero. O sistema falhava em identificar gaps críticos em documentos curtos/sintéticos e não localizava corretamente as normas no Knowledge Base (KB) quando referenciadas apenas por números.

## Decisões Técnicas

### 1. Normalização de Códigos
- Introduzida a função `sanitizeCode` para converter consistentemente entradas como "5", "NR-05" ou "NR 5" para o formato canônico do sistema de arquivos (`nr-5`).

### 2. Algoritmo de Ranking Híbrido
- O score de relevância foi ajustado para considerar tanto a proporção de tokens encontrados (`scoreProporcional`) quanto a contagem absoluta de hits (`hitsAbsolutos`). 
- Isso beneficia documentos curtos, onde uma única palavra-chave (ex: "CIPA") deve garantir alta relevância, mesmo que o chunk da norma seja longo.

### 3. Filtros e Stopwords
- Implementada filtragem de palavras comuns (stopwords) para evitar ruídos no ranking.
- Aumentado o `overlap` de chunks para preservar o contexto de sentenças cortadas.

### 4. Validação de Evidências
- A API agora valida se os `chunkIds` retornados pela IA existem no conjunto recuperado pelo KB (case-insensitive).
- Gaps sem evidências válidas são descartados para evitar alucinações e erros de integridade.

## Como Verificar
Execute o Scorecard de avaliação:
```bash
python3 scripts/harbor-scorecard.py
```
Meta de performance: Recall >= 0.90 em todos os casos do Golden Dataset (incluindo NR-29 e NR-30).
